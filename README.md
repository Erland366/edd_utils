# edd_utils: Enhanced Development & Debugging Utilities

`edd_utils` is a Python package designed to provide helpful utilities for development and debugging, particularly for PyTorch-based projects. This document focuses on its profiling capabilities.

## Profiling with `edd_utils`

The profiling tools within `edd_utils` aim to simplify performance analysis for PyTorch models, especially when working with the Hugging Face `Trainer`. The primary component for this is the `ProfCallback`.

### Using `ProfCallback`

`ProfCallback` is a Hugging Face `TrainerCallback` that leverages `torch.profiler` to collect and save detailed performance data during your training runs.

**Import and Instantiation:**

To use it, import `ProfCallback` from `edd_utils.main` and add it to your `Trainer`'s callbacks.

```python
from edd_utils.main import ProfCallback
from transformers import TrainingArguments, Trainer

# ... your model, dataset, tokenizer setup ...

# Example: Initialize ProfCallback for standard PyTorch profiling
prof_callback = ProfCallback(output_dir="./profiler_output")

# Add it to your TrainingArguments or directly to the Trainer
training_args = TrainingArguments(
    output_dir="./results",
    per_device_train_batch_size=8,
    num_train_epochs=1,
    # ... other essential training arguments ...
)

trainer = Trainer(
    model=model, # Your PyTorch model
    args=training_args,
    train_dataset=train_dataset, # Your training dataset
    # ... other trainer arguments ...
    callbacks=[prof_callback]
)

# Start training (profiler will be active based on its schedule)
# trainer.train() 
# Note: trainer.train() is commented out here to make this snippet runnable for illustration
# without actual model/data. Uncomment it in your actual training script.
```

**Key Parameters:**

The `ProfCallback` offers several parameters to customize profiling behavior:

*   **`output_dir: Optional[str]`**:
    *   Specifies the directory where profiling artifacts will be saved.
    *   Defaults to `"./profiler_output"` if not provided.

*   **Schedule Parameters:** These control when and how long the profiler is active.
    *   `wait_steps: Optional[int]`: Number of initial steps to ignore before the profiler warms up. Default: `4`.
    *   `warmup_steps: Optional[int]`: Number of steps for profiler warm-up after `wait_steps`. Data from these steps is not recorded. Default: `4`.
    *   `active_steps: Optional[int]`: Number of steps during which profiling data is actively recorded. This is a key parameter to control total profile size. Default: `1`.
    *   `num_cycles: Optional[int]`: Number of times the `wait -> warmup -> active` sequence is repeated. Default: `1`.

*   **Controlling Profile Size / Content:**
    These parameters from `torch.profiler` help manage the verbosity and size of the generated profiles. Setting them to `False` can significantly reduce output size, especially for large models or long runs.
    *   `profile_memory: bool`: Enables memory profiling. Default: `True`. Disabling this greatly reduces profile size.
    *   `with_stack: bool`: Includes stack traces in the profile. Default: `True`. Disabling this reduces profile size. (Note: `profile_memory=True` forces `with_stack=True`).
    *   `record_shapes: bool`: Records shapes of tensors. Default: `True`. (Note: `profile_memory=True` forces `record_shapes=True`).

*   **Selective Artifact Export:** These flags allow you to control which specific output files are generated by the default `trace_handler` (when not in Nsight Systems mode). Setting them to `False` can further reduce clutter and disk space usage if you only need certain artifacts.
    *   `export_memory_timeline: bool`: Whether to export the memory timeline HTML file and snapshot. Default: `True`.
    *   `export_stacks: bool`: Whether to export stack traces to a text file. Default: `True`.
    *   `export_key_averages: bool`: Whether to export the table of operator averages (e.g., `key_averages.txt`). Default: `True`.
    *(Note: The main Chrome/TensorBoard trace is always exported by the default handler).*

*   **Nsight Systems Integration:**
    *   `enable_nsight_systems: bool`: Set to `True` if you intend to profile your script using NVIDIA's Nsight Systems (`nsys`). Default: `False`.
    *   `nsight_systems_output_file: Optional[str]`: If `enable_nsight_systems` is `True` but the script is NOT detected to be running under `nsys profile`, this path is suggested in a warning message for the user to launch `nsys` with. Default: `None` (which results in a default like `"profiler_output/nsys_profile.nsys-rep"` being suggested).
    *   **Behavior with `enable_nsight_systems=True`:**
        *   **If `nsys` is active (script is run via `nsys profile ...`):** `ProfCallback` detects this (via the `NSYS_PROFILING_SESSION_ID` environment variable). It then configures the PyTorch profiler to primarily generate NVTX ranges, which `nsys` will capture. The standard PyTorch trace artifact generation by `ProfCallback`'s `trace_handler` (like Chrome traces, memory timelines) is disabled to avoid redundant data, as `nsys` will be creating its own comprehensive report.
        *   **If `nsys` is NOT active:** A warning message is printed to the console, guiding the user on how to launch the script with `nsys profile -o <path> python your_script.py`. The PyTorch profiler within `ProfCallback` is disabled in this case to prevent misleading or incomplete profiles, encouraging the user to use `nsys` as intended.

**Example Snippet (Conceptual Configurations):**

```python
# Conceptual example
from edd_utils.main import ProfCallback
from transformers import TrainingArguments, Trainer

# Assume model, train_dataset are defined
# class MyModel(torch.nn.Module): ...
# model = MyModel()
# train_dataset = ...

training_args = TrainingArguments(
    output_dir="./results",
    # Minimal viable training arguments for example
    per_device_train_batch_size=1,
    num_train_epochs=1,
    # Add other necessary arguments for your setup
)

# --- Choose one ProfCallback configuration ---

# Example 1: Basic PyTorch profiling with default settings
# prof_callback_default = ProfCallback(output_dir="./profiler_output_pytorch")

# Example 2: Configuration for potentially smaller/focused outputs
prof_callback_small = ProfCallback(
    output_dir="./profiler_output_small",
    active_steps=1,         # Shorter active profiling window
    profile_memory=False,   # Disable detailed memory profiling
    with_stack=False,       # Disable stack traces
    record_shapes=False,    # Disable tensor shape recording
    export_memory_timeline=False, # Don't save separate memory timeline HTML
    export_stacks=False,          # Don't save separate stacks file
    export_key_averages=True      # Still get the operator summary
)

# Example 3: Nsight Systems integration
# (Use this if you plan to run your script with `nsys profile ...`)
# prof_callback_nsys = ProfCallback(
#     output_dir="./profiler_output_nsys_aux", # Dir for any aux files if needed, nsys controls main output
#     enable_nsight_systems=True,
#     nsight_systems_output_file="my_model_profile.nsys-rep" # Suggested name for nsys output
# )

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    # Add the desired profiler callback configuration here
    callbacks=[prof_callback_small] # or prof_callback_default, or prof_callback_nsys
)

# To run training:
# print("Starting conceptual training run...")
# trainer.train() 
# print("Conceptual training run finished.")
# Note: trainer.train() is commented out here to make this snippet runnable for illustration
# without requiring a full model/data setup. Uncomment it in your actual training script.
```

### Advanced Profiling with NVIDIA Tools

For even deeper insights, especially into GPU kernel performance, `edd_utils` facilitates the use of NVIDIA's standalone profiling tools.

*   **Nsight Systems (`nsys`):**
    As mentioned, setting `enable_nsight_systems=True` in `ProfCallback` configures PyTorch to emit NVTX ranges that `nsys` can capture, providing a rich, system-level view of your application's execution timeline. This helps in understanding CPU/GPU interactions, API calls, and high-level kernel activity.

*   **Nsight Compute (`ncu`):**
    For in-depth analysis of individual CUDA kernels, Nsight Compute (`ncu`) is the recommended tool. After identifying performance-critical kernels (e.g., via `nsys` or the `key_averages.txt` from `ProfCallback`), `ncu` can provide detailed hardware counter information and optimization guidance.
    For detailed instructions on using `ncu`, see the [Nsight Compute Profiling Guide](docs/nsight_compute_guidance.md).

### Output Files

When using the standard PyTorch profiler (i.e., `enable_nsight_systems=False`), `ProfCallback` will generate several files in the specified `output_dir` (within a sub-directory like `iteration_X`):

*   **TensorBoard Trace:** A `.pt.trace.json` (or `.pt.trace.json.gz`) file that can be loaded into TensorBoard or `perfetto.dev` (chrome://tracing) to visualize the execution timeline. This is the primary output for general analysis.
*   **`key_averages.txt`:** A text file showing aggregated time spent in different operators (CPU and CUDA), grouped by operator name, input shapes, and stack traces. Very useful for identifying expensive operations or kernels. (Generation controlled by `export_key_averages`).
*   **`rankX_memory-timeline.html`:** An HTML file visualizing memory allocation events over time (if `profile_memory=True` and `export_memory_timeline=True`).
*   **`rankX_memory_snapshot.pickle`:** A pickle file containing memory snapshot data (if `profile_memory=True` and `export_memory_timeline=True`).
*   **`rankX_stacks.txt`:** A text file containing recorded stack traces for profiled events (if `with_stack=True` and `export_stacks=True`).

Remember that the generation of the memory timeline, stacks, and key averages files can be toggled using the `export_*` flags in `ProfCallback`. If `enable_nsight_systems=True` and `nsys` is active, these specific files from `ProfCallback` are typically not generated, as `nsys` will produce its own report file.
